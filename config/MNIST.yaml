define: &image_size 32
define: &embedding_size 8
define: &latent_size 8
define: &device 'cuda'
define: &vae_file 'VAE/models/MNIST.pth'
define: &save_file 'models/MNIST.pth'
device: *device


pretrain_encoder_params:
  image_size: *image_size
  image_channels: 1
  embedding_size: *embedding_size
  embedding_channels: 1
  code_book_size: 16
  h_channels: 16
  latent_channels: 4
  n_res_layers: 2
  layers: 2
  beta: 0.25
  device: *device
  
pretrain_training_params:
  learning_rate: 0.0003
  num_epochs: 30
  log_interval: 100
  vae_file: *vae_file
  
pretrain_sampler_params:
  image_size: 32
  batch_size: 128
  device: *device
  image_dir: 'datasets/mnist'

unet_params:
  image_size: *latent_size
  layers_per_block: 4
  block_out_channels: [128, 256, 256]
  down_block_types: ["DownBlock2D", "AttnDownBlock2D"]
  up_block_types: ["UpBlock2D", "AttnUpBlock2D"]
  unet_output_channels: 8
  down_channels: [ 128, 256, 256]
  mid_channels: [ 256]
  down_sample: [False, False]
  attn_down : [True, True]
  time_emb_dim: 256
  norm_channels : 32
  conv_out_channels : 32
  num_heads: 2
  num_down_layers: 2
  num_mid_layers: 2
  num_head_mid_layers: 4
  num_head_mid_blocks: 2
  num_up_layers: 2
  # condition_config:
  #   condition_types: ['class']
  #   class_condition_config :
  #     num_classes : 9
  #     cond_drop_prob : 0.1
  feature_out: False

score_training_params:
  learning_rate: 0.00003
  num_epochs: 500
  lr_warmpup_steps: 500
  mixed_precision: 'fp16'
  gradient_accumulation_steps: 1
  from_checkpoint: False
  vae_file: *vae_file
  save_file: *save_file
  checkpoint: *save_file
  display_file: 'images/score_training_3.png'
  device: *device

score_sampler_params:
  image_size: 32
  batch_size: 128
  device: *device
  image_dir: 'datasets/mnist'

test_sampler_params:
  image_size: 32
  batch_size: 4
  device: *device
  image_dir: 'datasets/mnist'
